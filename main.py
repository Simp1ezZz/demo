"""
LangChain Demo - ä½¿ç”¨ HuggingFace æœ¬åœ°æ¨¡å‹
å±•ç¤º LangChain 1.2+ çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œæ— éœ€ API Key
"""
import os
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»– HuggingFace æ¨¡å‹ï¼‰
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"  # å°æ¨¡å‹ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•
# MODEL_NAME = "THUDM/chatglm3-6b"  # ä¸­æ–‡æ•ˆæœæ›´å¥½çš„æ¨¡å‹
# MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"  # ä¸­ç­‰å¤§å°çš„æ¨¡å‹

# å…¨å±€ LLM å¯¹è±¡
llm = None

def init_model():
    """åˆå§‹åŒ– HuggingFace æ¨¡å‹"""
    global llm
    
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME}...")
    print("âš ï¸  é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚\n")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    
    # æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        low_cpu_mem_usage=True
    )
    
    if device == "cpu":
        model = model.to(device)
    
    # åˆ›å»º pipeline
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,
        return_full_text=False,
        device=0 if device == "cuda" else -1
    )
    
    # åˆ›å»º LangChain LLM
    llm = HuggingFacePipeline(pipeline=pipe)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n")


def demo_basic_llm():
    """æ¼”ç¤ºåŸºæœ¬çš„ LLM è°ƒç”¨"""
    print("=" * 50)
    print("æ¼”ç¤º 1: åŸºæœ¬çš„ LLM è°ƒç”¨")
    print("=" * 50)
    
    # ç®€å•è°ƒç”¨
    response = llm.invoke("ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
    print(f"å›ç­”: {response}\n")


def demo_prompt_template():
    """æ¼”ç¤ºä½¿ç”¨æç¤ºæ¨¡æ¿"""
    print("=" * 50)
    print("æ¼”ç¤º 2: ä½¿ç”¨æç¤ºæ¨¡æ¿")
    print("=" * 50)
    
    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_template(
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚\n\nè¯·è§£é‡Šä¸€ä¸‹ {technology} æ˜¯ä»€ä¹ˆï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå®é™…åº”ç”¨ä¾‹å­ã€‚"
    )
    
    # æ ¼å¼åŒ–æç¤º
    formatted_prompt = prompt.format(technology="åŒºå—é“¾")
    response = llm.invoke(formatted_prompt)
    print(f"å›ç­”: {response}\n")


def demo_chain():
    """æ¼”ç¤ºé“¾å¼è°ƒç”¨"""
    print("=" * 50)
    print("æ¼”ç¤º 3: é“¾å¼è°ƒç”¨ (LLMChain)")
    print("=" * 50)
    
    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_template(
        "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼Œå¹¶æ€»ç»“å…¶ä¸»è¦å†…å®¹ï¼š\n\n{text}"
    )
    
    # åˆ›å»ºé“¾
    chain = LLMChain(llm=llm, prompt=prompt)
    
    # æ‰§è¡Œé“¾
    result = chain.invoke({
        "text": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
    })
    
    print(f"ç»“æœ: {result['text']}\n")


def demo_multi_step():
    """æ¼”ç¤ºå¤šæ­¥éª¤å¤„ç†"""
    print("=" * 50)
    print("æ¼”ç¤º 4: å¤šæ­¥éª¤å¤„ç†")
    print("=" * 50)
    
    # æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜
    step1_prompt = ChatPromptTemplate.from_template(
        "åŸºäºä»¥ä¸‹å…³é”®è¯ç”Ÿæˆä¸€ä¸ªæŠ€æœ¯ä¸»é¢˜ï¼š{keywords}"
    )
    step1_chain = LLMChain(llm=llm, prompt=step1_prompt)
    
    # æ­¥éª¤2: åŸºäºä¸»é¢˜ç”Ÿæˆå†…å®¹
    step2_prompt = ChatPromptTemplate.from_template(
        "ä¸ºä¸»é¢˜ '{topic}' å†™ä¸€æ®µç®€çŸ­çš„ä»‹ç»ï¼ˆ100å­—ä»¥å†…ï¼‰"
    )
    step2_chain = LLMChain(llm=llm, prompt=step2_prompt)
    
    # æ‰§è¡Œå¤šæ­¥éª¤
    print("\næ­£åœ¨æ‰§è¡Œæ­¥éª¤1: ç”Ÿæˆä¸»é¢˜...")
    topic_result = step1_chain.invoke({"keywords": "æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ"})
    topic = topic_result['text'].strip()
    
    print("æ­£åœ¨æ‰§è¡Œæ­¥éª¤2: ç”Ÿæˆä¸»é¢˜ä»‹ç»...")
    content_result = step2_chain.invoke({"topic": topic})
    
    print(f"\nç”Ÿæˆçš„ä¸»é¢˜: {topic}")
    print(f"ä¸»é¢˜ä»‹ç»: {content_result['text']}\n")


def demo_conversation():
    """æ¼”ç¤ºå¯¹è¯å¼äº¤äº’"""
    print("=" * 50)
    print("æ¼”ç¤º 5: å¯¹è¯å¼äº¤äº’")
    print("=" * 50)
    
    # æ„å»ºå¯¹è¯æç¤º
    conversation_history = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”æŠ€æœ¯é—®é¢˜ã€‚\n\n"
    
    # ç¬¬ä¸€è½®å¯¹è¯
    print("\nç¬¬ä¸€è½®å¯¹è¯:")
    user_input1 = "ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ"
    prompt1 = conversation_history + f"ç”¨æˆ·: {user_input1}\nåŠ©æ‰‹:"
    response1 = llm.invoke(prompt1)
    print(f"ç”¨æˆ·: {user_input1}")
    print(f"åŠ©æ‰‹: {response1}\n")
    
    # ç¬¬äºŒè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
    print("ç¬¬äºŒè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰:")
    conversation_history += f"ç”¨æˆ·: {user_input1}\nåŠ©æ‰‹: {response1}\n\n"
    user_input2 = "å®ƒæœ‰ä»€ä¹ˆä¸»è¦ä¼˜åŠ¿ï¼Ÿ"
    prompt2 = conversation_history + f"ç”¨æˆ·: {user_input2}\nåŠ©æ‰‹:"
    response2 = llm.invoke(prompt2)
    print(f"ç”¨æˆ·: {user_input2}")
    print(f"åŠ©æ‰‹: {response2}\n")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 50)
    print("LangChain 1.2+ Demo - HuggingFace æœ¬åœ°æ¨¡å‹")
    print("=" * 50 + "\n")
    
    try:
        # åˆå§‹åŒ–æ¨¡å‹
        init_model()
        
        # è¿è¡Œå„ä¸ªæ¼”ç¤º
        demo_basic_llm()
        demo_prompt_template()
        demo_chain()
        demo_multi_step()
        demo_conversation()
        
        print("=" * 50)
        print("æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("\nè¯·ç¡®ä¿ï¼š")
        print("1. å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
        print("2. ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼‰")
        print("3. æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆæ¨¡å‹æ–‡ä»¶å¯èƒ½è¾ƒå¤§ï¼‰")
        print(f"4. å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {MODEL_NAME}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
