{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain 1.2+ Demo - ä½¿ç”¨ HuggingFace æœ¬åœ°æ¨¡å‹\n",
        "\n",
        "è¿™ä¸ª notebook å±•ç¤ºäº† LangChain 1.2+ ç‰ˆæœ¬ä¸ HuggingFace æœ¬åœ°æ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•ã€‚\n",
        "\n",
        "**æ— éœ€ API Keyï¼** æ‰€æœ‰æ¨¡å‹éƒ½åœ¨æœ¬åœ°è¿è¡Œã€‚\n",
        "\n",
        "## åŠŸèƒ½æ¼”ç¤º\n",
        "\n",
        "1. **åŸºæœ¬çš„ LLM è°ƒç”¨** - å±•ç¤ºå¦‚ä½•ä½¿ç”¨ LangChain è°ƒç”¨æœ¬åœ° HuggingFace æ¨¡å‹\n",
        "2. **æç¤ºæ¨¡æ¿** - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æç¤ºæ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
        "3. **é“¾å¼è°ƒç”¨** - å±•ç¤º LLMChain çš„ä½¿ç”¨æ–¹æ³•\n",
        "4. **å¤šæ­¥éª¤å¤„ç†** - æ¼”ç¤ºå¦‚ä½•å°†å¤šä¸ªæ­¥éª¤ä¸²è”èµ·æ¥\n",
        "5. **å¯¹è¯å¼äº¤äº’** - å±•ç¤ºå¦‚ä½•ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡\n",
        "\n",
        "---\n",
        "\n",
        "## æ­¥éª¤ 1: å®‰è£…ä¾èµ–\n",
        "\n",
        "é¦–å…ˆéœ€è¦å®‰è£… LangChain 1.2+ ç‰ˆæœ¬å’Œ HuggingFace ç›¸å…³ä¾èµ–ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£… LangChain 1.2 å’Œ HuggingFace ç›¸å…³ä¾èµ–\n",
        "!pip install -q langchain>=1.2.0 langchain-community>=0.2.0 langchain-core>=0.2.0\n",
        "!pip install -q transformers>=4.35.0 torch>=2.0.0 accelerate>=0.24.0 sentencepiece>=0.1.99\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤ 2: é€‰æ‹©å¹¶åŠ è½½ HuggingFace æ¨¡å‹\n",
        "\n",
        "é€‰æ‹©ä½ æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚è¿™é‡Œæä¾›äº†å‡ ä¸ªé€‰é¡¹ï¼š\n",
        "\n",
        "- **Qwen2.5-0.5B** (æ¨èï¼Œå°æ¨¡å‹ï¼Œé€Ÿåº¦å¿«) - `Qwen/Qwen2.5-0.5B-Instruct`\n",
        "- **ChatGLM3-6B** (ä¸­æ–‡æ•ˆæœå¥½) - `THUDM/chatglm3-6b`\n",
        "- **Qwen2.5-1.5B** (å¹³è¡¡é€‰æ‹©) - `Qwen/Qwen2.5-1.5B-Instruct`\n",
        "- **å…¶ä»–æ¨¡å‹** - å¯ä»¥æ›¿æ¢ä¸ºä»»ä½• HuggingFace ä¸Šçš„æ¨¡å‹\n",
        "\n",
        "**æ³¨æ„ï¼š** é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ã€‚æ¨¡å‹ä¼šç¼“å­˜åœ¨æœ¬åœ°ï¼Œåç»­è¿è¡Œä¼šæ›´å¿«ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»– HuggingFace æ¨¡å‹ï¼‰\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # å°æ¨¡å‹ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•\n",
        "# MODEL_NAME = \"THUDM/chatglm3-6b\"  # ä¸­æ–‡æ•ˆæœæ›´å¥½çš„æ¨¡å‹\n",
        "# MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"  # ä¸­ç­‰å¤§å°çš„æ¨¡å‹\n",
        "\n",
        "print(f\"âœ… å°†ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}\")\n",
        "print(\"ğŸ“¥ é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ­¥éª¤ 3: å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆå§‹åŒ–æ¨¡å‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"âœ… æ‰€æœ‰åº“å·²å¯¼å…¥ï¼\")\n",
        "print(f\"âœ… ä½¿ç”¨ LangChain 1.2+ ç‰ˆæœ¬\")\n",
        "print(f\"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
        "\n",
        "# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
        "print(f\"\\nğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹ {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# æ ¹æ®è®¾å¤‡é€‰æ‹©åŠ è½½æ–¹å¼\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model = model.to(device)\n",
        "\n",
        "# åˆ›å»º pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        "    return_full_text=False,\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "# åˆ›å»º LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å¯ä»¥å¼€å§‹ä½¿ç”¨äº†ã€‚\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## æ¼”ç¤º 1: åŸºæœ¬çš„ LLM è°ƒç”¨\n",
        "\n",
        "è¿™æ˜¯æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼Œç›´æ¥è°ƒç”¨ LLM å¹¶è·å–å›ç­”ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"æ¼”ç¤º 1: åŸºæœ¬çš„ LLM è°ƒç”¨\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ç®€å•è°ƒç”¨ï¼ˆæ³¨æ„ï¼šHuggingFacePipeline è¿”å›å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¯¹è±¡ï¼‰\n",
        "response = llm.invoke(\"ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ\")\n",
        "print(f\"\\nå›ç­”: {response}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ¼”ç¤º 2: ä½¿ç”¨æç¤ºæ¨¡æ¿\n",
        "\n",
        "æç¤ºæ¨¡æ¿å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç»„ç»‡å’Œæ ¼å¼åŒ–è¾“å…¥ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦åŠ¨æ€å†…å®¹çš„åœºæ™¯ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"æ¼”ç¤º 2: ä½¿ç”¨æç¤ºæ¨¡æ¿\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# åˆ›å»ºæç¤ºæ¨¡æ¿ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²æ¨¡æ¿ï¼Œå› ä¸º HuggingFacePipeline æ¥å—å­—ç¬¦ä¸²ï¼‰\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯é¡¾é—®ï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚\\n\\nè¯·è§£é‡Šä¸€ä¸‹ {technology} æ˜¯ä»€ä¹ˆï¼Œå¹¶ç»™å‡ºä¸€ä¸ªå®é™…åº”ç”¨ä¾‹å­ã€‚\"\n",
        ")\n",
        "\n",
        "# æ ¼å¼åŒ–æç¤º\n",
        "formatted_prompt = prompt.format(technology=\"åŒºå—é“¾\")\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(f\"\\nå›ç­”: {response}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ¼”ç¤º 3: é“¾å¼è°ƒç”¨ (LLMChain)\n",
        "\n",
        "LLMChain æ˜¯ LangChain çš„æ ¸å¿ƒæ¦‚å¿µä¹‹ä¸€ï¼Œå®ƒå°†æç¤ºæ¨¡æ¿å’Œ LLM ç»„åˆæˆä¸€ä¸ªå¯é‡ç”¨çš„é“¾ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"æ¼”ç¤º 3: é“¾å¼è°ƒç”¨ (LLMChain)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# åˆ›å»ºæç¤ºæ¨¡æ¿\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼Œå¹¶æ€»ç»“å…¶ä¸»è¦å†…å®¹ï¼š\\n\\n{text}\"\n",
        ")\n",
        "\n",
        "# åˆ›å»ºé“¾\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# æ‰§è¡Œé“¾\n",
        "result = chain.invoke({\n",
        "    \"text\": \"äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚\"\n",
        "})\n",
        "\n",
        "print(f\"\\nç»“æœ: {result['text']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ¼”ç¤º 4: å¤šæ­¥éª¤å¤„ç†\n",
        "\n",
        "æœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦å°†å¤šä¸ªæ“ä½œä¸²è”èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå¤„ç†æµç¨‹ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"æ¼”ç¤º 4: å¤šæ­¥éª¤å¤„ç†\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# æ­¥éª¤1: ç”Ÿæˆä¸»é¢˜\n",
        "step1_prompt = ChatPromptTemplate.from_template(\n",
        "    \"åŸºäºä»¥ä¸‹å…³é”®è¯ç”Ÿæˆä¸€ä¸ªæŠ€æœ¯ä¸»é¢˜ï¼š{keywords}\"\n",
        ")\n",
        "step1_chain = LLMChain(llm=llm, prompt=step1_prompt)\n",
        "\n",
        "# æ­¥éª¤2: åŸºäºä¸»é¢˜ç”Ÿæˆå†…å®¹\n",
        "step2_prompt = ChatPromptTemplate.from_template(\n",
        "    \"ä¸ºä¸»é¢˜ '{topic}' å†™ä¸€æ®µç®€çŸ­çš„ä»‹ç»ï¼ˆ100å­—ä»¥å†…ï¼‰\"\n",
        ")\n",
        "step2_chain = LLMChain(llm=llm, prompt=step2_prompt)\n",
        "\n",
        "# æ‰§è¡Œå¤šæ­¥éª¤\n",
        "print(\"\\næ­£åœ¨æ‰§è¡Œæ­¥éª¤1: ç”Ÿæˆä¸»é¢˜...\")\n",
        "topic_result = step1_chain.invoke({\"keywords\": \"æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ , ç¥ç»ç½‘ç»œ\"})\n",
        "topic = topic_result['text'].strip()\n",
        "\n",
        "print(\"æ­£åœ¨æ‰§è¡Œæ­¥éª¤2: ç”Ÿæˆä¸»é¢˜ä»‹ç»...\")\n",
        "content_result = step2_chain.invoke({\"topic\": topic})\n",
        "\n",
        "print(f\"\\nç”Ÿæˆçš„ä¸»é¢˜: {topic}\")\n",
        "print(f\"ä¸»é¢˜ä»‹ç»: {content_result['text']}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## æ¼”ç¤º 5: å¯¹è¯å¼äº¤äº’\n",
        "\n",
        "è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œå®ç°å¤šè½®å¯¹è¯ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"æ¼”ç¤º 5: å¯¹è¯å¼äº¤äº’\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# æ„å»ºå¯¹è¯æç¤ºï¼ˆHuggingFacePipeline ä½¿ç”¨å­—ç¬¦ä¸²è¾“å…¥ï¼‰\n",
        "conversation_history = \"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”æŠ€æœ¯é—®é¢˜ã€‚\\n\\n\"\n",
        "\n",
        "# ç¬¬ä¸€è½®å¯¹è¯\n",
        "print(\"\\nç¬¬ä¸€è½®å¯¹è¯:\")\n",
        "user_input1 = \"ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ\"\n",
        "prompt1 = conversation_history + f\"ç”¨æˆ·: {user_input1}\\nåŠ©æ‰‹:\"\n",
        "response1 = llm.invoke(prompt1)\n",
        "print(f\"ç”¨æˆ·: {user_input1}\")\n",
        "print(f\"åŠ©æ‰‹: {response1}\\n\")\n",
        "\n",
        "# ç¬¬äºŒè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰\n",
        "print(\"ç¬¬äºŒè½®å¯¹è¯ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰:\")\n",
        "conversation_history += f\"ç”¨æˆ·: {user_input1}\\nåŠ©æ‰‹: {response1}\\n\\n\"\n",
        "user_input2 = \"å®ƒæœ‰ä»€ä¹ˆä¸»è¦ä¼˜åŠ¿ï¼Ÿ\"\n",
        "prompt2 = conversation_history + f\"ç”¨æˆ·: {user_input2}\\nåŠ©æ‰‹:\"\n",
        "response2 = llm.invoke(prompt2)\n",
        "print(f\"ç”¨æˆ·: {user_input2}\")\n",
        "print(f\"åŠ©æ‰‹: {response2}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ‰ å®Œæˆï¼\n",
        "\n",
        "æ‰€æœ‰æ¼”ç¤ºå·²å®Œæˆã€‚ä½ å¯ä»¥ï¼š\n",
        "\n",
        "1. **ä¿®æ”¹å‚æ•°** - å°è¯•ä¸åŒçš„ temperatureã€model ç­‰å‚æ•°\n",
        "2. **è‡ªå®šä¹‰æç¤º** - ä¿®æ”¹æç¤ºæ¨¡æ¿æ¥é€‚åº”ä½ çš„éœ€æ±‚\n",
        "3. **æ‰©å±•åŠŸèƒ½** - æ·»åŠ æ›´å¤šæ­¥éª¤æˆ–é›†æˆå…¶ä»– LangChain ç»„ä»¶\n",
        "\n",
        "### ä¸‹ä¸€æ­¥æ¢ç´¢\n",
        "\n",
        "- å‘é‡æ•°æ®åº“é›†æˆï¼ˆå¦‚ Chromaã€Pineconeï¼‰\n",
        "- å·¥å…·å’Œä»£ç†ï¼ˆAgentsï¼‰\n",
        "- æ–‡æ¡£åŠ è½½å’Œå¤„ç†\n",
        "- è®°å¿†ï¼ˆMemoryï¼‰åŠŸèƒ½\n",
        "- æµå¼è¾“å‡ºï¼ˆStreamingï¼‰\n",
        "\n",
        "### å‚è€ƒèµ„æº\n",
        "\n",
        "- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)\n",
        "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
